{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled5.ipynb","provenance":[],"authorship_tag":"ABX9TyNt4IXlFz8bkEFbDrjJ+LM4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L7dWKL1h1j8F","executionInfo":{"status":"ok","timestamp":1655443434145,"user_tz":-330,"elapsed":10970,"user":{"displayName":"Shubham Badgire","userId":"15643814326347250815"}},"outputId":"5a694518-8bff-4215-80fe-7ed45fbf7221"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.3.0)\n","Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.5)\n","openjdk-8-jdk-headless is already the newest version (8u312-b07-0ubuntu1~18.04).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"]}],"source":["!pip install pyspark\n","!pip install -U -q PyDrive\n","!apt install openjdk-8-jdk-headless -qq\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark import SparkContext, SparkConf\n"],"metadata":{"id":"gI8-7dBj2AjH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create the session\n","conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n","\n","# create the context\n","import pyspark\n","sc = pyspark.SparkContext(conf=conf)\n","spark = SparkSession.builder.getOrCreate()\n"],"metadata":{"id":"Tvg0Ce_U2JAV","executionInfo":{"status":"error","timestamp":1655443442778,"user_tz":-330,"elapsed":601,"user":{"displayName":"Shubham Badgire","userId":"15643814326347250815"}},"outputId":"0d019946-8590-4382-e782-77670910bdcc","colab":{"base_uri":"https://localhost:8080/","height":259}},"execution_count":10,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-7115d681a7e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# create the context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\u001b[0m\n\u001b[1;32m    193\u001b[0m             )\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m             self._do_init(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    437\u001b[0m                             \u001b[0mcallsite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                             \u001b[0mcallsite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                             \u001b[0mcallsite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinenum\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m                         )\n\u001b[1;32m    441\u001b[0m                     )\n","\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-3-7115d681a7e4>:6 "]}]},{"cell_type":"code","source":["!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","!unzip ngrok-stable-linux-amd64.zip\n","get_ipython().system_raw('./ngrok http 4050 &')\n","!sleep 10\n","!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n","    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U3P28UL82MAV","executionInfo":{"status":"ok","timestamp":1655438913185,"user_tz":-330,"elapsed":12295,"user":{"displayName":"Shubham Badgire","userId":"15643814326347250815"}},"outputId":"d8d55dc3-3bb0-4d3c-9dd0-924c31bbd910"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-06-17 04:08:20--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n","Resolving bin.equinox.io (bin.equinox.io)... 52.202.168.65, 54.237.133.81, 54.161.241.46, ...\n","Connecting to bin.equinox.io (bin.equinox.io)|52.202.168.65|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 13832437 (13M) [application/octet-stream]\n","Saving to: ‘ngrok-stable-linux-amd64.zip’\n","\n","ngrok-stable-linux- 100%[===================>]  13.19M  18.0MB/s    in 0.7s    \n","\n","2022-06-17 04:08:21 (18.0 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13832437/13832437]\n","\n","Archive:  ngrok-stable-linux-amd64.zip\n","  inflating: ngrok                   \n","https://f387-34-125-72-220.ngrok.io\n"]}]},{"cell_type":"code","source":["from pyspark.sql.functions import approx_count_distinct,collect_list\n","from pyspark.sql.functions import collect_set,sum,avg,max,countDistinct,count\n","from pyspark.sql.functions import first, last, min, mean \n","from pyspark.sql.functions import sumDistinct"],"metadata":{"id":"vfBR0rqg4BuP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["simpleData = [(\"James\", \"Sales\", 3000),\n","    (\"Michael\", \"Sales\", 4600),\n","    (\"Robert\", \"Sales\", 4100),\n","    (\"Maria\", \"Finance\", 3000),\n","    (\"James\", \"Sales\", 3000),\n","    (\"Scott\", \"Finance\", 3300),\n","    (\"Jen\", \"Finance\", 3900),\n","    (\"Jeff\", \"Marketing\", 3000),\n","    (\"Kumar\", \"Marketing\", 2000),\n","    (\"Saif\", \"Sales\", 4100)\n","  ]\n","schema = [\"employee_name\", \"department\", \"salary\"]"],"metadata":{"id":"YoN5Dqhl4Fhi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = spark.createDataFrame(data=simpleData, schema = schema)"],"metadata":{"id":"C2PonCZH4OqX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","df.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Ds-v1Wk4YJ4","executionInfo":{"status":"ok","timestamp":1655439473363,"user_tz":-330,"elapsed":3311,"user":{"displayName":"Shubham Badgire","userId":"15643814326347250815"}},"outputId":"a638143e-ce0c-40fc-c02b-6f91b9261dd8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+-------------+----------+------+\n","|employee_name|department|salary|\n","+-------------+----------+------+\n","|        James|     Sales|  3000|\n","|      Michael|     Sales|  4600|\n","|       Robert|     Sales|  4100|\n","|        Maria|   Finance|  3000|\n","|        James|     Sales|  3000|\n","|        Scott|   Finance|  3300|\n","|          Jen|   Finance|  3900|\n","|         Jeff| Marketing|  3000|\n","|        Kumar| Marketing|  2000|\n","|         Saif|     Sales|  4100|\n","+-------------+----------+------+\n","\n"]}]}]}